---
title: "NPSS"
author: "Ross Kelly"
date: "30/03/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Net Promoter Score and Confidence Intervals: A cautionary tale

Those who have undertaken quantitative consumer research to understand competitive positioning may well have come across this scenario. Your team has run a consumer survey to understand Consumer Key Purchasing Criteria (KPCs) and competitive brand positioning. The CEO wants to proclaim that our brand is better than our major competitor, since our Net Promoter Score (NPS) is higher! There's just one problem - it's for a serious fundraising round, and the rest of the Exec are concerned that we could be overstating our advantage: 

_"Is there a way we can say that the difference in Net Promoter Score is_ __statistically significant?"__

A statistically significant difference is when, at a given level of confidence, we reject the null hypothesis that two sample statistics are derived from the same population, and that there is a genuine difference in performance between the two sample groups. Commercially, this means that we believe that the difference we see between our two brands is reflective of a real difference in Net Promoter Score, rather than sampling error.

OK - problem solved then? Well, NPS is a compound statistic, with very wide confidence intervals, as we will see below. We will cover a way to artificially get round this (by bootstrapping our sample), but fundamentally NPS is always going to provide problems for data scientists. A much better option is to advise stakeholders to run 'overall rating' scores. That being said, NPS is a universally understood metric, so let's see how we can make it work as best we can. 

### So why does NPS pose problems for statistical analysis?

NPS is calculated by asking customers how likely they would be to recommend the brand to a friend or colleague, on a scale from 0 to 10. Respondents who score 9 or 10 are considered _Promoters_, those who score 7 or 8 are _Passives_, and those who score 6 or less are considered _Detractors_. The % of all respondents who are Promoters less the % who are Detractors is our Net Promoter Score:

$$ 
Net\ Promoter\ Score = \% Promoters - \% Detractors
$$
This creates a challenge for us when calculating _statistical differences_. Normally, we compare sample means or sample proportions, but at first glance NPS doesn't appear to be anything of the sort - it is the difference in proportions of two groups. However, we can re-arrange our NPS formula to generate a sample mean.

Thinking back to how we previously formulated NPS, it can also be calculated as follows:

$$
Net\ Promoter\ Score = \frac{Number\ of\ Promoters}{Total\ Number\ of\ Respondents} - \frac{Number\ of\ Detractors}{Total\ Number\ of\ Respondents} 
$$
Promoters are all considered equal (a 9 is valued the same as a 10), as are Detractors (responding with a 6 is considered to be as _dissatisfied_ as someone scoring 0). Ignoring the inherent issues in that for a second, this means that we could replace respondents' numeric scores (between 0 and 10) with a categorical variable, or a pair of dummies: a 0 or 1 flag for _Promoter_ and _Detractor_ (where 0 in both is our reference category - _Passives_).

Our NPS stat can now be calculated as follows:

$$
Net\ Promoter\ Score = \frac{Number\ of\ Promoter\ flags}{Total\ Number\ of\ Respondents} - \frac{Number\ of\ Detractors\ flags}{Total\ Number\ of\ Respondents} 
$$

This is effectively the difference between the mean of the Promoter flag, and the mean of the Detractor flag. Since the difference between means is the same as mean difference, this can be rewritten as follows, meaning we can map Promoter scores to 1, Passives to 0, and Detractors to -1. The mean of this transformed NPS score will also return the Net Promoter Score, which will as ever be a value between +1 and -1.

$$
Net\ Promoter\ Score = \frac{Number\ of\ Promoter\ flags\ -\ Number\ of\ Detractor\ flags}{Total\ Number\ of\ Respondents} = Mean(Net\ Promoter\ Score\ Flag)
$$

### Implications for Statistical Analysis

By being able to transform the raw scores into something that can be computed as a mean, then we can also compute standard deviation, and therefore confidence intervals and run hypothesis tests. Our survey NPS can be seen as a sample mean.

Our transformed metric will have a mean that sits between -1 and +1, but our variance will be relatively large - every promoter or detractor returns a max or min value! See the example below with a sample of 100 respondents, where there are 30 Promoters and 20 Detractors:

```{r echo= FALSE}
n <- 100
n_promoters_brand_a <- 30
n_detractors_brand_a <- 20
n_passives_brand_a <- n - n_promoters_brand_a - n_detractors_brand_a

promoter_sample_brand_a <- rep(1, times = n_promoters_brand_a)
passive_sample_brand_a <- rep(0, times = n_passives_brand_a)
detractor_sample_brand_a <- rep(-1, times = n_detractors_brand_a)
brand_a <- c(promoter_sample_brand_a, passive_sample_brand_a, detractor_sample_brand_a)
```

#### Net Promoter Score (as %)

```{r echo = FALSE, comment = ''}
cat(paste0(round(mean(brand_a),2)*100,"%"))
```

#### NPS Variance (as %)

```{r echo = FALSE, comment = ''}
cat(paste0(round(var(brand_a),2)*100,"%"))
```

Wow - we have an NPS of 10%, but the variance is 49%! This is perhaps unsurprising, given that respondents' scores are only ever +1 (the max), 0, or -1 (the min). Consequently, confidence intervals are quite wide:

```{r echo = FALSE, comment = ''}
library(ggplot2)
library(scales)
ci_mean <- data.frame(vals = c(mean(brand_a) - 1.96*(sd(brand_a)/sqrt(length(brand_a))), mean(brand_a), mean(brand_a) + 1.96*(sd(brand_a)/sqrt(length(brand_a)))), labels = c('Brand A Lower CI (95%)', 'Brand A NPS Statistic', 'Brand A Upper CI (95%)'))

d <- ggplot() + aes(brand_a) + 
  geom_density() + geom_vline(data = ci_mean, aes(xintercept= vals), color="blue", size=0.5) + geom_text(data = ci_mean, aes(x=vals, y = 0.5, label = labels), size = 4, angle = 90, vjust = -0.4, ) + ggtitle(label = "Transformed NPS with NPS statistic and Confidence Intervals (95%)") + scale_x_continuous(labels = percent)
d
```

Our survey sample NPS statistic is 10%, but 95% confidence intervals suggest that the _true population NPS statistic_ (NPS if you were to ask the **entire addressable market** instead of a survey sample) is anywhere between -6% and 28%!

This makes it impossible to determine a significant difference between our brand and our competitor, who have an NPS of -5% from our survey (again n = 100, this time there are 36 Promoters and 32 Detractors).

```{r echo= FALSE, warning = FALSE}
library(stringr)
library(ggplot2)
library(scales)
n <- 100
n_promoters_brand_b <- 30
n_detractors_brand_b <- 35
n_passives_brand_b <- n - n_promoters_brand_b - n_detractors_brand_b


promoter_sample_brand_b <- rep(1, times = n_promoters_brand_b)
passive_sample_brand_b <- rep(0, times = n_passives_brand_b)
detractor_sample_brand_b <- rep(-1, times = n_detractors_brand_b)
brand_b <- c(promoter_sample_brand_b, passive_sample_brand_b, detractor_sample_brand_b)
npss_brand_b <- mean(brand_b)

ci_mean_a <- data.frame(vals = c(mean(brand_a) - 1.96*(sd(brand_a)/sqrt(length(brand_a))), mean(brand_a), mean(brand_a) + 1.96*(sd(brand_a)/sqrt(length(brand_a)))), labels = c('Brand A Lower CI (95%)', 'Brand A NPS Statistic', 'Brand A Upper CI (95%)'))

ci_mean_b <- data.frame(vals = c(mean(brand_b) - 1.96*(sd(brand_a)/sqrt(length(brand_b))), mean(brand_b), mean(brand_b) + 1.96*(sd(brand_b)/sqrt(length(brand_b)))), labels = c('Brand B Lower CI (95%)', 'Brand B NPS Statistic', 'Brand B Upper CI (95%)'))

cis <- data.table::as.data.table(rbind(ci_mean_a, ci_mean_b))
cis[, brand := stringr::str_extract(labels, regex("Brand [AB]"))]
ciscast <- cis
ciscast[, labels:= gsub("^Brand [AB] ","",labels)]
ciscast <- data.table::dcast(ciscast, value.var = "vals", brand ~ labels)
names(ciscast) <- tolower(gsub(" ","_", names(ciscast)))
ciscast$brand <- as.factor(ciscast$brand)

b <- ggplot() + geom_point(data = ciscast, aes(x = brand, y = ciscast$nps_statistic, color = ciscast$brand),size = 5) + geom_errorbar(aes(x = ciscast$brand, ymin = ciscast$`lower_ci_(95%)`, ymax = ciscast$`upper_ci_(95%)`)) + ggplot2::ylab("NPS Statistic with Confidence Intervals (95%)") + xlab("Brand") + labs(colour = "Brands", title = "NPS and Confidence Intervals (95%) for Brands A and B") + scale_y_continuous(labels = percent)
b
##ggplot() + geom_crossbar(data = ciscast, aes(x = "nps_statistic", xmin = "lower_ci_(95%)", xmax = ##"upper_ci_(95%)", y = "brand"))
```

When confidence intervals are overlapping, it is impossible for us to reject the null hypothesis that there is in fact no meaningful difference between our brands; our CEO can't make his claim that we're more popular. But is there a way we can reduce the width of our confidence intervals so that they cease to overlap?

### Bootstrapping to reduce Confidence Interval Width

Confidence intervals are derived from the following formula:

$$
Confidence\ Interval\ = \overline{x} \pm z\frac{s}{\sqrt n}
$$
To reduce the width of our confidence intervals around the sample mean $\overline{x}$, we need to reduce the width of the curve. We don't want to change the degree of confidence and therefore the $z$ value (1.96 at 5% alpha - our type I error), and we aren't going to be able to reduce our variance, since the transformed version of NPS only has values of 1, 0 or -1. However, there may be a way to increase the denominator, by bootstrapping the sample.

Bootstrapping involves sampling _from your sample_ with replacement. Imagine you have a box full of 10,000 red and blue marbles (your population), and you don't know the exact ratio of red to blue, so you take a sample of 100 marbles to estimate this, and place them in a bag. The bag (your sample) is the equivalent to our survey. The bag is believed to be representative of the total box of marbles, but in order to better build confidence about your sample mean, you randomly sample the bag (taking a marble out, recording its colour, and returning it to the bag) and note the ratio of reds to blues you record with this _increasing_ sample of marbles. The idea is that, with multiple samples _of the sample_, we will settle on a stable sample ratio that is closer to the true population proportion. This act of sampling with replacement effectively increases the sample size of our survey.

By increasing the sample size, the denominator is increased, thereby reducing the overall width of our confidence intervals.

Below we can see the same samples, but with bootstrapping with replacement so that our N is 500 instead of 100:

```{r echo= FALSE, warning = FALSE}
library(stringr)
library(ggplot2)
library(scales)

set.seed(45)

brand_a_bs <- sample(brand_a, 500, replace = T)
brand_b_bs <- sample(brand_b, 500, replace = T)

ci_mean_a_bs <- data.frame(vals = c(mean(brand_a_bs) - 1.96*(sd(brand_a_bs)/sqrt(length(brand_a_bs))), mean(brand_a_bs), mean(brand_a_bs) + 1.96*(sd(brand_a_bs)/sqrt(length(brand_a_bs)))), labels = c('Brand A Lower CI (95%)', 'Brand A NPS Statistic', 'Brand A Upper CI (95%)'))

ci_mean_b_bs <- data.frame(vals = c(mean(brand_b_bs) - 1.96*(sd(brand_b_bs)/sqrt(length(brand_b_bs))), mean(brand_b_bs), mean(brand_b_bs) + 1.96*(sd(brand_b_bs)/sqrt(length(brand_b_bs)))), labels = c('Brand B Lower CI (95%)', 'Brand B NPS Statistic', 'Brand B Upper CI (95%)'))

cis_bs <- data.table::as.data.table(rbind(ci_mean_a_bs, ci_mean_b_bs))
cis_bs[, brand := stringr::str_extract(labels, regex("Brand [AB]"))]
ciscast_bs <- cis_bs
ciscast_bs[, labels:= gsub("^Brand [AB] ","",labels)]
ciscast_bs <- data.table::dcast(ciscast_bs, value.var = "vals", brand ~ labels)
names(ciscast_bs) <- tolower(gsub(" ","_", names(ciscast_bs)))
ciscast_bs$brand <- as.factor(ciscast_bs$brand)

c <- ggplot() + geom_point(data = ciscast_bs, aes(x = brand, y = ciscast_bs$nps_statistic, color = ciscast_bs$brand),size = 5) + geom_errorbar(aes(x = ciscast_bs$brand, ymin = ciscast_bs$`lower_ci_(95%)`, ymax = ciscast_bs$`upper_ci_(95%)`)) + ggplot2::ylab("NPS Statistic with Confidence Intervals (95%)") + xlab("Brand") + labs(colour = "Brands", title = "Bootstrapped NPS and Confidence Intervals (95%) for Brands A and B", subtitle = "Original Sample 100, Bootstrapped N of 500") + scale_y_continuous(labels = percent)
c

```

Wow! bootstrapping has considerably reduced the width of our Confidence Intervals, and we can now see a clear, significant difference between the two brands! Note that the sampling with replacement has also slightly effected our sample means

Whilst this can be achieved by a simple T test for two brands (**NB:this should nearly always be a Welch test - see this great article by XXX on why**), this becomes more tricky when there are multiple brands to consider. The way to achieve this is a post-hoc test of honest significant difference. For a straight Student T test we can use Tukey's Significant Difference Test, for running a post-hoc test on a Welch T test we need to look at the Games-Howell post-hoc test.